{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Res34_cifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11c6e86b6abb457e9b48446febbb14b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f75f6f84abd442e4bb1938e2b1272834",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e859bb136f04d0e852bd373800e687a",
              "IPY_MODEL_4f341cee4eca44bdbd22254b62dbd599"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "f75f6f84abd442e4bb1938e2b1272834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "4e859bb136f04d0e852bd373800e687a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_144555ce1b1645079233a018e7faade6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7131e6219b2445fb0513e53ae8544e6"
          },
          "model_module_version": "1.5.0"
        },
        "4f341cee4eca44bdbd22254b62dbd599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd2b95d7d2594fc7a47c0b55fe1276d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [05:07&lt;00:00, 554052.36it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2afac878f5e345bf8ba36c9ccce87050"
          },
          "model_module_version": "1.5.0"
        },
        "144555ce1b1645079233a018e7faade6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "b7131e6219b2445fb0513e53ae8544e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "bd2b95d7d2594fc7a47c0b55fe1276d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "2afac878f5e345bf8ba36c9ccce87050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhramit/csa2022/blob/main/Res34_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZM_TB5kgW9O"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m6DmHpIgqU4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "11c6e86b6abb457e9b48446febbb14b7",
            "f75f6f84abd442e4bb1938e2b1272834",
            "4e859bb136f04d0e852bd373800e687a",
            "4f341cee4eca44bdbd22254b62dbd599",
            "144555ce1b1645079233a018e7faade6",
            "b7131e6219b2445fb0513e53ae8544e6",
            "bd2b95d7d2594fc7a47c0b55fe1276d0",
            "2afac878f5e345bf8ba36c9ccce87050"
          ]
        },
        "id": "LBzsWY2bgsZ8",
        "outputId": "77c5a684-b288-4841-8997-18cae51d3c63"
      },
      "source": [
        "transform_train= transforms.Compose([\n",
        "    transforms.RandomCrop(32,padding=2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
        "transform_test= transforms.Compose([  \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10('./datasets', train=True,download=True, transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10('./datasets', train=False,download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128,shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128,shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11c6e86b6abb457e9b48446febbb14b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enH0Q19eioYO"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-CYDpwf0FHd"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n",
        "        self.classifer = nn.Linear(512 * block.expansion, num_classes)\n",
        "        \n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifer(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P07OcOshWDXC"
      },
      "source": [
        "this is block repetition for res34\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIjiwWxR0KW7"
      },
      "source": [
        "resnet=ResNet(BasicBlock, [3,4,6,3]).to(device)\n",
        "resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p7dMV9eWn41"
      },
      "source": [
        "lr = 1e-1\n",
        "loss_func= nn.CrossEntropyLoss()\n",
        "optimiser= torch.optim.Adam(resnet.parameters(),lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, threshold=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_BW3Rn50iOu"
      },
      "source": [
        "\n",
        "def train(epoch,):\n",
        "    print('\\nEpoch: %d' % (epoch))\n",
        "    resnet.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimiser.zero_grad()\n",
        "        outputs = resnet(inputs)\n",
        "        loss = loss_func(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx % 100 == 99:    \n",
        "            print('[%d, %5d] loss: %.6f |  Acc: %.3f%% (%d/%d)' %\n",
        "                  (epoch + 1, batch_idx + 1, train_loss, 100.*correct/total, correct, total))\n",
        "    return train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maGCnd-u8d7C",
        "outputId": "9c09c36b-6c6e-4cac-c8d7-26fbfbb817db"
      },
      "source": [
        "for epoch in range(0,50):\n",
        "    loss = train(epoch)\n",
        "    print('Total loss: %.6f' % loss)\n",
        "    scheduler.step(loss, epoch=epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 185.626995 |  Acc: 31.352% (4013/12800)\n",
            "[1,   200] loss: 352.982768 |  Acc: 34.527% (8839/25600)\n",
            "[1,   300] loss: 508.687939 |  Acc: 37.195% (14283/38400)\n",
            "Total loss: 641.935596\n",
            "\n",
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2,   100] loss: 139.121776 |  Acc: 49.000% (6272/12800)\n",
            "[2,   200] loss: 271.006103 |  Acc: 50.195% (12850/25600)\n",
            "[2,   300] loss: 396.693378 |  Acc: 51.557% (19798/38400)\n",
            "Total loss: 507.040579\n",
            "\n",
            "Epoch: 2\n",
            "[3,   100] loss: 112.775806 |  Acc: 59.336% (7595/12800)\n",
            "[3,   200] loss: 219.744841 |  Acc: 60.547% (15500/25600)\n",
            "[3,   300] loss: 324.024507 |  Acc: 61.156% (23484/38400)\n",
            "Total loss: 414.204898\n",
            "\n",
            "Epoch: 3\n",
            "[4,   100] loss: 93.073471 |  Acc: 66.734% (8542/12800)\n",
            "[4,   200] loss: 183.828107 |  Acc: 67.125% (17184/25600)\n",
            "[4,   300] loss: 274.379137 |  Acc: 67.612% (25963/38400)\n",
            "Total loss: 351.963298\n",
            "\n",
            "Epoch: 4\n",
            "[5,   100] loss: 81.128208 |  Acc: 71.688% (9176/12800)\n",
            "[5,   200] loss: 159.045742 |  Acc: 72.066% (18449/25600)\n",
            "[5,   300] loss: 236.024195 |  Acc: 72.315% (27769/38400)\n",
            "Total loss: 303.674363\n",
            "\n",
            "Epoch: 5\n",
            "[6,   100] loss: 68.994550 |  Acc: 75.898% (9715/12800)\n",
            "[6,   200] loss: 136.533165 |  Acc: 76.180% (19502/25600)\n",
            "[6,   300] loss: 203.389879 |  Acc: 76.430% (29349/38400)\n",
            "Total loss: 262.688247\n",
            "\n",
            "Epoch: 6\n",
            "[7,   100] loss: 60.134570 |  Acc: 79.062% (10120/12800)\n",
            "[7,   200] loss: 120.496834 |  Acc: 78.848% (20185/25600)\n",
            "[7,   300] loss: 179.891668 |  Acc: 78.961% (30321/38400)\n",
            "Total loss: 233.109088\n",
            "\n",
            "Epoch: 7\n",
            "[8,   100] loss: 53.725170 |  Acc: 81.445% (10425/12800)\n",
            "[8,   200] loss: 107.796148 |  Acc: 81.289% (20810/25600)\n",
            "[8,   300] loss: 162.236576 |  Acc: 81.177% (31172/38400)\n",
            "Total loss: 210.354083\n",
            "\n",
            "Epoch: 8\n",
            "[9,   100] loss: 47.860333 |  Acc: 83.375% (10672/12800)\n",
            "[9,   200] loss: 96.094099 |  Acc: 83.461% (21366/25600)\n",
            "[9,   300] loss: 145.637133 |  Acc: 83.234% (31962/38400)\n",
            "Total loss: 188.802150\n",
            "\n",
            "Epoch: 9\n",
            "[10,   100] loss: 43.263113 |  Acc: 85.164% (10901/12800)\n",
            "[10,   200] loss: 86.998860 |  Acc: 85.051% (21773/25600)\n",
            "[10,   300] loss: 132.704233 |  Acc: 84.724% (32534/38400)\n",
            "Total loss: 172.849643\n",
            "\n",
            "Epoch: 10\n",
            "[11,   100] loss: 38.265641 |  Acc: 86.656% (11092/12800)\n",
            "[11,   200] loss: 78.525163 |  Acc: 86.309% (22095/25600)\n",
            "[11,   300] loss: 119.187657 |  Acc: 86.208% (33104/38400)\n",
            "Total loss: 156.266771\n",
            "\n",
            "Epoch: 11\n",
            "[12,   100] loss: 34.386887 |  Acc: 88.203% (11290/12800)\n",
            "[12,   200] loss: 70.590596 |  Acc: 87.832% (22485/25600)\n",
            "[12,   300] loss: 105.749194 |  Acc: 87.826% (33725/38400)\n",
            "Total loss: 139.268113\n",
            "\n",
            "Epoch: 12\n",
            "[13,   100] loss: 31.606648 |  Acc: 89.133% (11409/12800)\n",
            "[13,   200] loss: 63.942790 |  Acc: 88.844% (22744/25600)\n",
            "[13,   300] loss: 98.193124 |  Acc: 88.638% (34037/38400)\n",
            "Total loss: 129.979971\n",
            "\n",
            "Epoch: 13\n",
            "[14,   100] loss: 29.237854 |  Acc: 89.852% (11501/12800)\n",
            "[14,   200] loss: 58.909244 |  Acc: 89.852% (23002/25600)\n",
            "[14,   300] loss: 91.085471 |  Acc: 89.448% (34348/38400)\n",
            "Total loss: 118.623159\n",
            "\n",
            "Epoch: 14\n",
            "[15,   100] loss: 26.443636 |  Acc: 91.023% (11651/12800)\n",
            "[15,   200] loss: 54.541464 |  Acc: 90.488% (23165/25600)\n",
            "[15,   300] loss: 82.823884 |  Acc: 90.401% (34714/38400)\n",
            "Total loss: 108.758164\n",
            "\n",
            "Epoch: 15\n",
            "[16,   100] loss: 24.072963 |  Acc: 91.570% (11721/12800)\n",
            "[16,   200] loss: 49.144039 |  Acc: 91.297% (23372/25600)\n",
            "[16,   300] loss: 74.187693 |  Acc: 91.396% (35096/38400)\n",
            "Total loss: 98.175019\n",
            "\n",
            "Epoch: 16\n",
            "[17,   100] loss: 20.631158 |  Acc: 92.992% (11903/12800)\n",
            "[17,   200] loss: 43.535137 |  Acc: 92.578% (23700/25600)\n",
            "[17,   300] loss: 66.787904 |  Acc: 92.339% (35458/38400)\n",
            "Total loss: 88.283031\n",
            "\n",
            "Epoch: 17\n",
            "[18,   100] loss: 19.818080 |  Acc: 93.141% (11922/12800)\n",
            "[18,   200] loss: 40.159938 |  Acc: 93.066% (23825/25600)\n",
            "[18,   300] loss: 61.887916 |  Acc: 92.823% (35644/38400)\n",
            "Total loss: 81.863922\n",
            "\n",
            "Epoch: 18\n",
            "[19,   100] loss: 18.566504 |  Acc: 93.500% (11968/12800)\n",
            "[19,   200] loss: 39.299241 |  Acc: 93.215% (23863/25600)\n",
            "[19,   300] loss: 60.456909 |  Acc: 93.003% (35713/38400)\n",
            "Total loss: 78.255231\n",
            "\n",
            "Epoch: 19\n",
            "[20,   100] loss: 17.736162 |  Acc: 94.062% (12040/12800)\n",
            "[20,   200] loss: 36.151361 |  Acc: 93.820% (24018/25600)\n",
            "[20,   300] loss: 55.130832 |  Acc: 93.651% (35962/38400)\n",
            "Total loss: 71.461364\n",
            "\n",
            "Epoch: 20\n",
            "[21,   100] loss: 15.777675 |  Acc: 94.500% (12096/12800)\n",
            "[21,   200] loss: 32.285454 |  Acc: 94.465% (24183/25600)\n",
            "[21,   300] loss: 48.796189 |  Acc: 94.398% (36249/38400)\n",
            "Total loss: 63.997341\n",
            "\n",
            "Epoch: 21\n",
            "[22,   100] loss: 13.975640 |  Acc: 95.195% (12185/12800)\n",
            "[22,   200] loss: 29.629323 |  Acc: 94.961% (24310/25600)\n",
            "[22,   300] loss: 44.906235 |  Acc: 94.833% (36416/38400)\n",
            "Total loss: 60.739691\n",
            "\n",
            "Epoch: 22\n",
            "[23,   100] loss: 12.768716 |  Acc: 95.883% (12273/12800)\n",
            "[23,   200] loss: 28.161413 |  Acc: 95.379% (24417/25600)\n",
            "[23,   300] loss: 43.045102 |  Acc: 95.266% (36582/38400)\n",
            "Total loss: 57.056573\n",
            "\n",
            "Epoch: 23\n",
            "[24,   100] loss: 13.303661 |  Acc: 95.508% (12225/12800)\n",
            "[24,   200] loss: 26.543372 |  Acc: 95.488% (24445/25600)\n",
            "[24,   300] loss: 39.961433 |  Acc: 95.490% (36668/38400)\n",
            "Total loss: 52.474508\n",
            "\n",
            "Epoch: 24\n",
            "[25,   100] loss: 12.893223 |  Acc: 95.719% (12252/12800)\n",
            "[25,   200] loss: 25.419977 |  Acc: 95.699% (24499/25600)\n",
            "[25,   300] loss: 40.656422 |  Acc: 95.391% (36630/38400)\n",
            "Total loss: 52.655415\n",
            "\n",
            "Epoch: 25\n",
            "[26,   100] loss: 11.628862 |  Acc: 96.211% (12315/12800)\n",
            "[26,   200] loss: 24.956479 |  Acc: 95.910% (24553/25600)\n",
            "[26,   300] loss: 37.752076 |  Acc: 95.815% (36793/38400)\n",
            "Total loss: 48.992145\n",
            "\n",
            "Epoch: 26\n",
            "[27,   100] loss: 9.552068 |  Acc: 96.789% (12389/12800)\n",
            "[27,   200] loss: 20.340774 |  Acc: 96.566% (24721/25600)\n",
            "[27,   300] loss: 32.683879 |  Acc: 96.284% (36973/38400)\n",
            "Total loss: 44.753414\n",
            "\n",
            "Epoch: 27\n",
            "[28,   100] loss: 10.241599 |  Acc: 96.555% (12359/12800)\n",
            "[28,   200] loss: 21.871132 |  Acc: 96.344% (24664/25600)\n",
            "[28,   300] loss: 32.721046 |  Acc: 96.286% (36974/38400)\n",
            "Total loss: 43.401465\n",
            "\n",
            "Epoch: 28\n",
            "[29,   100] loss: 9.754211 |  Acc: 96.742% (12383/12800)\n",
            "[29,   200] loss: 20.097675 |  Acc: 96.586% (24726/25600)\n",
            "[29,   300] loss: 29.906539 |  Acc: 96.591% (37091/38400)\n",
            "Total loss: 40.336306\n",
            "\n",
            "Epoch: 29\n",
            "[30,   100] loss: 8.711776 |  Acc: 97.133% (12433/12800)\n",
            "[30,   200] loss: 17.663911 |  Acc: 97.070% (24850/25600)\n",
            "[30,   300] loss: 27.025413 |  Acc: 96.971% (37237/38400)\n",
            "Total loss: 37.722129\n",
            "\n",
            "Epoch: 30\n",
            "[31,   100] loss: 9.498886 |  Acc: 96.719% (12380/12800)\n",
            "[31,   200] loss: 19.585744 |  Acc: 96.547% (24716/25600)\n",
            "[31,   300] loss: 29.375640 |  Acc: 96.555% (37077/38400)\n",
            "Total loss: 39.750766\n",
            "\n",
            "Epoch: 31\n",
            "[32,   100] loss: 6.664358 |  Acc: 97.758% (12513/12800)\n",
            "[32,   200] loss: 15.543952 |  Acc: 97.336% (24918/25600)\n",
            "[32,   300] loss: 24.681742 |  Acc: 97.169% (37313/38400)\n",
            "Total loss: 33.956079\n",
            "\n",
            "Epoch: 32\n",
            "[33,   100] loss: 9.952537 |  Acc: 96.625% (12368/12800)\n",
            "[33,   200] loss: 18.603806 |  Acc: 96.797% (24780/25600)\n",
            "[33,   300] loss: 28.219281 |  Acc: 96.758% (37155/38400)\n",
            "Total loss: 37.265845\n",
            "\n",
            "Epoch: 33\n",
            "[34,   100] loss: 7.915189 |  Acc: 97.047% (12422/12800)\n",
            "[34,   200] loss: 17.197170 |  Acc: 96.922% (24812/25600)\n",
            "[34,   300] loss: 26.386775 |  Acc: 96.935% (37223/38400)\n",
            "Total loss: 34.887222\n",
            "\n",
            "Epoch: 34\n",
            "[35,   100] loss: 6.501154 |  Acc: 97.852% (12525/12800)\n",
            "[35,   200] loss: 14.652222 |  Acc: 97.574% (24979/25600)\n",
            "[35,   300] loss: 23.108719 |  Acc: 97.445% (37419/38400)\n",
            "Total loss: 30.683892\n",
            "\n",
            "Epoch: 35\n",
            "[36,   100] loss: 7.182092 |  Acc: 97.633% (12497/12800)\n",
            "[36,   200] loss: 14.779622 |  Acc: 97.512% (24963/25600)\n",
            "[36,   300] loss: 22.757672 |  Acc: 97.430% (37413/38400)\n",
            "Total loss: 30.667990\n",
            "\n",
            "Epoch: 36\n",
            "[37,   100] loss: 6.406264 |  Acc: 97.805% (12519/12800)\n",
            "[37,   200] loss: 13.996594 |  Acc: 97.574% (24979/25600)\n",
            "[37,   300] loss: 24.373126 |  Acc: 97.286% (37358/38400)\n",
            "Total loss: 31.217733\n",
            "\n",
            "Epoch: 37\n",
            "[38,   100] loss: 6.417809 |  Acc: 97.930% (12535/12800)\n",
            "[38,   200] loss: 14.036311 |  Acc: 97.742% (25022/25600)\n",
            "[38,   300] loss: 21.659114 |  Acc: 97.667% (37504/38400)\n",
            "Total loss: 28.939033\n",
            "\n",
            "Epoch: 38\n",
            "[39,   100] loss: 5.887160 |  Acc: 98.031% (12548/12800)\n",
            "[39,   200] loss: 12.180960 |  Acc: 97.957% (25077/25600)\n",
            "[39,   300] loss: 19.116684 |  Acc: 97.893% (37591/38400)\n",
            "Total loss: 26.435715\n",
            "\n",
            "Epoch: 39\n",
            "[40,   100] loss: 6.175075 |  Acc: 97.844% (12524/12800)\n",
            "[40,   200] loss: 12.262054 |  Acc: 97.906% (25064/25600)\n",
            "[40,   300] loss: 20.872833 |  Acc: 97.659% (37501/38400)\n",
            "Total loss: 28.208196\n",
            "\n",
            "Epoch: 40\n",
            "[41,   100] loss: 5.936941 |  Acc: 97.875% (12528/12800)\n",
            "[41,   200] loss: 11.776268 |  Acc: 97.945% (25074/25600)\n",
            "[41,   300] loss: 18.000305 |  Acc: 97.883% (37587/38400)\n",
            "Total loss: 24.320135\n",
            "\n",
            "Epoch: 41\n",
            "[42,   100] loss: 5.157983 |  Acc: 98.164% (12565/12800)\n",
            "[42,   200] loss: 11.430325 |  Acc: 97.984% (25084/25600)\n",
            "[42,   300] loss: 19.987886 |  Acc: 97.747% (37535/38400)\n",
            "Total loss: 26.362040\n",
            "\n",
            "Epoch: 42\n",
            "[43,   100] loss: 6.799645 |  Acc: 97.781% (12516/12800)\n",
            "[43,   200] loss: 13.549272 |  Acc: 97.715% (25015/25600)\n",
            "[43,   300] loss: 20.374925 |  Acc: 97.753% (37537/38400)\n",
            "Total loss: 26.377797\n",
            "\n",
            "Epoch: 43\n",
            "[44,   100] loss: 5.582719 |  Acc: 98.141% (12562/12800)\n",
            "[44,   200] loss: 10.832865 |  Acc: 98.215% (25143/25600)\n",
            "[44,   300] loss: 17.555609 |  Acc: 98.073% (37660/38400)\n",
            "Total loss: 24.609934\n",
            "\n",
            "Epoch: 44\n",
            "[45,   100] loss: 6.208992 |  Acc: 97.961% (12539/12800)\n",
            "[45,   200] loss: 12.811326 |  Acc: 97.902% (25063/25600)\n",
            "[45,   300] loss: 19.772028 |  Acc: 97.857% (37577/38400)\n",
            "Total loss: 26.248954\n",
            "\n",
            "Epoch: 45\n",
            "[46,   100] loss: 6.141312 |  Acc: 97.891% (12530/12800)\n",
            "[46,   200] loss: 13.867282 |  Acc: 97.707% (25013/25600)\n",
            "[46,   300] loss: 20.409268 |  Acc: 97.802% (37556/38400)\n",
            "Total loss: 26.854898\n",
            "\n",
            "Epoch: 46\n",
            "[47,   100] loss: 4.623713 |  Acc: 98.477% (12605/12800)\n",
            "[47,   200] loss: 10.300914 |  Acc: 98.277% (25159/25600)\n",
            "[47,   300] loss: 16.468489 |  Acc: 98.148% (37689/38400)\n",
            "Total loss: 22.321149\n",
            "\n",
            "Epoch: 47\n",
            "[48,   100] loss: 4.760289 |  Acc: 98.391% (12594/12800)\n",
            "[48,   200] loss: 10.204346 |  Acc: 98.273% (25158/25600)\n",
            "[48,   300] loss: 16.970607 |  Acc: 98.096% (37669/38400)\n",
            "Total loss: 22.231598\n",
            "\n",
            "Epoch: 48\n",
            "[49,   100] loss: 4.102568 |  Acc: 98.602% (12621/12800)\n",
            "[49,   200] loss: 10.018492 |  Acc: 98.305% (25166/25600)\n",
            "[49,   300] loss: 17.545328 |  Acc: 98.052% (37652/38400)\n",
            "Total loss: 23.678110\n",
            "\n",
            "Epoch: 49\n",
            "[50,   100] loss: 5.360317 |  Acc: 98.352% (12589/12800)\n",
            "[50,   200] loss: 10.383128 |  Acc: 98.297% (25164/25600)\n",
            "[50,   300] loss: 15.910104 |  Acc: 98.260% (37732/38400)\n",
            "Total loss: 21.826383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dCh8YmY9muj",
        "outputId": "5c04864e-be36-4eb5-e8db-f2b5bd4c505c"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "outputs = resnet(images.to(device))\n",
        "_, predicted = torch.max(outputs.cpu(), 1)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = resnet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 86 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijbdazkqd2U8",
        "outputId": "4c235b87-9a50-4db4-9fcd-c97497a0c506"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(resnet,(3,32,32))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-9           [-1, 64, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "             ReLU-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-15           [-1, 64, 32, 32]               0\n",
            "           Conv2d-16           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-17           [-1, 64, 32, 32]             128\n",
            "             ReLU-18           [-1, 64, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-21           [-1, 64, 32, 32]               0\n",
            "           Conv2d-22          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "             ReLU-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-26          [-1, 128, 16, 16]             256\n",
            "           Conv2d-27          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-29          [-1, 128, 16, 16]               0\n",
            "           Conv2d-30          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
            "             ReLU-32          [-1, 128, 16, 16]               0\n",
            "           Conv2d-33          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-34          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-35          [-1, 128, 16, 16]               0\n",
            "           Conv2d-36          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-37          [-1, 128, 16, 16]             256\n",
            "             ReLU-38          [-1, 128, 16, 16]               0\n",
            "           Conv2d-39          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-40          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-41          [-1, 128, 16, 16]               0\n",
            "           Conv2d-42          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
            "             ReLU-44          [-1, 128, 16, 16]               0\n",
            "           Conv2d-45          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-47          [-1, 128, 16, 16]               0\n",
            "           Conv2d-48            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-49            [-1, 256, 8, 8]             512\n",
            "             ReLU-50            [-1, 256, 8, 8]               0\n",
            "           Conv2d-51            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-52            [-1, 256, 8, 8]             512\n",
            "           Conv2d-53            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-54            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-55            [-1, 256, 8, 8]               0\n",
            "           Conv2d-56            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-57            [-1, 256, 8, 8]             512\n",
            "             ReLU-58            [-1, 256, 8, 8]               0\n",
            "           Conv2d-59            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-61            [-1, 256, 8, 8]               0\n",
            "           Conv2d-62            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-63            [-1, 256, 8, 8]             512\n",
            "             ReLU-64            [-1, 256, 8, 8]               0\n",
            "           Conv2d-65            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-67            [-1, 256, 8, 8]               0\n",
            "           Conv2d-68            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 8, 8]             512\n",
            "             ReLU-70            [-1, 256, 8, 8]               0\n",
            "           Conv2d-71            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-72            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-73            [-1, 256, 8, 8]               0\n",
            "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
            "             ReLU-76            [-1, 256, 8, 8]               0\n",
            "           Conv2d-77            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-78            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-79            [-1, 256, 8, 8]               0\n",
            "           Conv2d-80            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
            "             ReLU-82            [-1, 256, 8, 8]               0\n",
            "           Conv2d-83            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-84            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-85            [-1, 256, 8, 8]               0\n",
            "           Conv2d-86            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-87            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-88            [-1, 512, 4, 4]               0\n",
            "           Conv2d-89            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-90            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-91            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-92            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-93            [-1, 512, 4, 4]               0\n",
            "           Conv2d-94            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-95            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-96            [-1, 512, 4, 4]               0\n",
            "           Conv2d-97            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-99            [-1, 512, 4, 4]               0\n",
            "          Conv2d-100            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-101            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-102            [-1, 512, 4, 4]               0\n",
            "          Conv2d-103            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 4, 4]           1,024\n",
            "      BasicBlock-105            [-1, 512, 4, 4]               0\n",
            "       AvgPool2d-106            [-1, 512, 1, 1]               0\n",
            "          Linear-107                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 21,282,122\n",
            "Trainable params: 21,282,122\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 23.00\n",
            "Params size (MB): 81.18\n",
            "Estimated Total Size (MB): 104.20\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}